<!DOCTYPE html>
<html>
<head>
    <title>Project 5</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            margin: 20px;
        }
    </style>
</head>

<body>
  <h1>Project 5</h1>
  <h2>Fun With Diffusion Models</h2>

  <a href="#a0">Part A.0</a> <br>
  <a href="#a1.1">Part A.1.1</a> <br>
  <a href="#a1.2">Part A.1.2</a> <br>
  <a href="#a1.3">Part A.1.3</a> <br>
  <a href="#a1.4">Part A.1.4</a> <br>
  <a href="#a1.5">Part A.1.5</a> <br>
  <a href="#a1.6">Part A.1.6</a> <br>
  <a href="#a1.7">Part A.1.7</a> <br>
  <a href="#a1.7.1">Part A.1.7.1</a> <br>
  <a href="#a1.7.2">Part A.1.7.2</a> <br>
  <a href="#a1.7.3">Part A.1.7.3</a> <br>
  <a href="#a1.8">Part A.1.8</a> <br>
  <a href="#a1.9">Part A.1.9</a> <br>
  <br>
  <br>

  <h3>Part A: The Power of Diffusion Models</h3>
  <h4 id="a0">Part A.0: Setup</h4>
  <p>The first step of prompting the the DeepFloyd IF diffusion model is to convert text prompts into high-dimension vector embeddings, which are directly interpretable by the model. To do so, I used the DeepFloyd IF T5 text encoder convert my text prompts into embeddings. Here are my prompts for random images, visual anagrams, and hybrid images:</p>
  <ul>
    <li>A photo of a student studying at the library</li>
    <li>A photo of a barista making a coffee</li>
    <li>A photo of a cat lying upside down playing with a ball of yarn</li>
    <li>A photo of warm weather</li>
    <li>A photo that looks like a cat when oriented one way, but looks like a dog when flipped vertically</li>
    <li>A black and white photo that looks like a man, but when the colors are inverted, looks like a woman</li>
    <li>A photo that looks like daytime oriented one way, but looks like nightime when flipped horizontally</li>
    <li>A photo that looks like a man smiling when black and white, but looks like a man frowning when in color</li>
    <li>A photo that looks like an old woman when in color, but looks like a young woman in black and white</li>
    <li>A photo that look like a city when still, but looks like a farm when in motion (blurred)</li>
  </ul>
  <p>To test the model on these prompts, I chose three of the embeddings and generated images from them. For all of these tasks, I used a random seed of 12102025. The following are the results:</p>
  <table border="1" cellpadding="4" >
    <thead>
        <tr>
            <th>Prompt / Caption</th>
            <th>Image</th>
            <th>Analysis</th>
        </tr>
    </thead>
    <tbody>
        <tr>
            <td>A photo of a student studying at the library</td>
            <td><img src="images/student.png"></td>
            <td>Here, the prompt is moderately specific and descriptive of the desired output. Consequently, the resulting image matches the intent of the prompt pretty well. One thing to note is that since the background is very detailed (libraries have hundreds of small detailed books and objects around them), the background quality is limited in comparison to the other images.</td>
        </tr>
        <tr>
            <td>A photo of warm weather</td>
            <td><img src="images/warm_weather.png"></td>
            <td>The prompt is somewhat abstract, since "warm weather" can look like a number of different things. Regardless, the model is still able to produce an image that matches the prompt, though it may differ a bit more from the user's intented results by nature of the vagueness of the prompt.</td>
        </tr>
        <tr>
            <td>A photo of a cat lying upside down playing with a ball of yarn</td>
            <td><img src="images/cat.png"></td>
            <td>This prompt is a lot more specific than the one about warm weather. The details of the cat and the patterned yarn are captured quite well, and it is likely that this is the kind of image the user was aiming to receive. One interesting observation is that the cat is not "lying upside down," though perhaps it is ambiguous as to what this exactly means.</td>
        </tr>
    </tbody>
  </table>
  <p>All of the above images were generated with 20 inference steps. To test the impact of the number of inference steps, I regenerated the image of the student with 200 inference steps instead, which resulted in the following image:</p>
  <img src="images/student_more_steps.png">
  <p>We can see that there are many more details in the image itself, which are specifically visible in the background (individual books, shelves, symbols, etc.) and in the objects (laptop keys and jacket folds, for example). This is due to the increase in the number of inference steps.</p>

  <br>
  <h4>PART A.1: SAMPLING LOOPS</h4>
  <h4 id="a1.1">Part A.1.1: Implementing the Forward Process</h4>
  <p>I implemented the diffusion forward process, which adds noise to the original (clean) image. This is the result of forward processing an image of the Campanile at various time steps:</p>
  <p>Left to right: t = 0 (original), 250, 500, 750</p>
  <img src="images/campanile_resized.png" height="200">
  <img src="images/campanile_250.png" height="200">
  <img src="images/campanile_500.png" height="200">
  <img src="images/campanile_750.png" height="200">

  <h4 id="a1.2">Part A.1.2: Classical Denoising</h4>
  <p>The results of attempting to denoise the above images using Gaussian blurring are shown below (left to right: t = 250, 500, 750):</p>
  <img src="images/campanile_250_gblur.png" height="200">
  <img src="images/campanile_500_gblur.png" height="200">
  <img src="images/campanile_750_gblur.png" height="200">

  <h4 id="a1.3">Part A.1.3: One-Step Denoising</h4>
  <p>Next, I attempted to denoise the noisy images using one-step denoising with the pretrained diffusion model by reversing the forward process. The results are shown below (left to right: t = 250, 500, 750; top to bottom: original image, noisy image, denoised image):</p>
  <img src="images/campanile_resized.png" width="200">
  <img src="images/campanile_resized.png" width="200">
  <img src="images/campanile_resized.png" width="200">
  <br>
  <img src="images/campanile_250_renoise.png" height="200">
  <img src="images/campanile_500_renoise.png" height="200">
  <img src="images/campanile_750_renoise.png" height="200">
  <br>
  <img src="images/campanile_250_denoise.png" height="200">
  <img src="images/campanile_500_denoise.png" height="200">
  <img src="images/campanile_750_denoise.png" height="200">

  <h4 id="a1.4">Part A.1.4: Iterative Denoising</h4>
  <p>The final method for denoising is iterative denoising. The following is a visualization of the noisy Campanile image every fifth loop of denoising, which gradually get less noisy expected (from right to left, as t decreases). From left to right, the time steps are t = 90, 240, 390, 540, 690:</p>
  <img src="images/campanile_iterative_90.png" height="200">
  <img src="images/campanile_iterative_240.png" height="200">
  <img src="images/campanile_iterative_390.png" height="200">
  <img src="images/campanile_iterative_540.png" height="200">
  <img src="images/campanile_iterative_690.png" height="200">
  <p>The following is a comparison of the original image (leftmost) to the three implemented denoising methods (left to right: iterative, one-step, and gaussian blur). As expected, iterative denoising performs the best:</p>
  <img src="images/campanile_resized.png" height="200">
  <img src="images/campanile_iterative_result.png" height="200">
  <img src="images/campanile_one_step_result.png" height="200">
  <img src="images/campanile_gblur_result.png" height="200">

  <h4 id="a1.5">Part A.1.5: Diffusion Model Sampling</h4>
  <p>The following is the result of 5 random samples from noise using the prompt "a high quality photo":</p>
  <img src="images/sample1.png" height="100">
  <img src="images/sample2.png" height="100">
  <img src="images/sample3.png" height="100">
  <img src="images/sample4.png" height="100">
  <img src="images/sample5.png" height="100">

  <h4 id="a1.6">Part A.1.6: Classifier-Free Guidance (CFG)</h4>
  <p>The following is the result of 5 random samples from noise using CFG with scale 7:</p>
  <img src="images/sample_cfg1.png" height="100">
  <img src="images/sample_cfg2.png" height="100">
  <img src="images/sample_cfg3.png" height="100">
  <img src="images/sample_cfg4.png" height="100">
  <img src="images/sample_cfg5.png" height="100">

  <h4 id="a1.7">Part A.1.7: Image-to-image Translation</h4>
  <p>The following is the result of edits (via CFG iterative denoising) to the Campanile at noise levels 1, 3, 5, 7, 10, and 20 (left to right), followed by the original image of the Campanile (rightmost). As expected, they increasingly look like the original Campanile:</p>
  <img src="images/im2im_campanile_1.png" height="200">
  <img src="images/im2im_campanile_3.png" height="200">
  <img src="images/im2im_campanile_5.png" height="200">
  <img src="images/im2im_campanile_7.png" height="200">
  <img src="images/im2im_campanile_10.png" height="200">
  <img src="images/im2im_campanile_20.png" height="200">
  <img src="images/campanile_resized.png" height="200">
  <p>The same procedure is applied to an image of a coffee mug, showing noise levels 1, 3, 5, 7, 10, and 20 (left to right) and the original image of the Campanile (rightmost):</p>
  <img src="images/im2im_coffee_1.png" height="200">
  <img src="images/im2im_coffee_3.png" height="200">
  <img src="images/im2im_coffee_5.png" height="200">
  <img src="images/im2im_coffee_7.png" height="200">
  <img src="images/im2im_coffee_10.png" height="200">
  <img src="images/im2im_coffee_20.png" height="200">
  <img src="images/im2im_coffee_og.png" height="200">
  <p>Finally, the same visualization is shown for an image of a balloon (same ordering):</p>
  <img src="images/im2im_balloon_1.png" height="200">
  <img src="images/im2im_balloon_3.png" height="200">
  <img src="images/im2im_balloon_5.png" height="200">
  <img src="images/im2im_balloon_7.png" height="200">
  <img src="images/im2im_balloon_10.png" height="200">
  <img src="images/im2im_balloon_20.png" height="200">
  <img src="images/im2im_balloon_og.png" height="200">

  <h4 id="a1.7.1">Part A.1.7.1: Editing Hand-Drawn and Web Images</h4>
  <p>The same SDEdit algorithm is applied to the following drawing of a banana from the internet (same ordering):</p>
  <img src="images/im2im_banana_1.png" height="200">
  <img src="images/im2im_banana_3.png" height="200">
  <img src="images/im2im_banana_5.png" height="200">
  <img src="images/im2im_banana_7.png" height="200">
  <img src="images/im2im_banana_10.png" height="200">
  <img src="images/im2im_banana_20.png" height="200">
  <img src="images/im2im_banana_og.png" height="200">
  <p>The following is the same result on a hand-drawing of a house:</p>
  <img src="images/im2im_house_1.png" height="200">
  <img src="images/im2im_house_3.png" height="200">
  <img src="images/im2im_house_5.png" height="200">
  <img src="images/im2im_house_7.png" height="200">
  <img src="images/im2im_house_10.png" height="200">
  <img src="images/im2im_house_20.png" height="200">
  <img src="images/im2im_house_og.png" height="200">
   <p>The following is the same result on a hand-drawing of a butterfly:</p>
  <img src="images/im2im_butterfly_1.png" height="200">
  <img src="images/im2im_butterfly_3.png" height="200">
  <img src="images/im2im_butterfly_5.png" height="200">
  <img src="images/im2im_butterfly_7.png" height="200">
  <img src="images/im2im_butterfly_10.png" height="200">
  <img src="images/im2im_butterfly_20.png" height="200">
  <img src="images/im2im_butterfly_og.png" height="200">

  <h4 id="a1.7.2">Part A.1.7.2: Inpainting</h4>
  <p>I implemented the logic for inpainting, resulting in the following results for the campanile:</p>
  <img src="images/campanile_resized.png" height="200">
  <img src="images/campanile_masked.png" height="200">
  <img src="images/campanile_with_mask.png" height="200">
  <img src="images/campanile_inpainted.png" height="200">
  <p>Inpainting results for an image of a tree:</p>
  <img src="images/tree.jpeg" height="200">
  <img src="images/tree_masked.png" height="200">
  <img src="images/tree_with_mask.png" height="200">
  <img src="images/tree_inpainted.png" height="200">
  <p>Inpainting results for an image of a dog:</p>
  <img src="images/dog.jpeg" height="200">
  <img src="images/dog_masked.png" height="200">
  <img src="images/dog_with_mask.png" height="200">
  <img src="images/dog_inpainted.png" height="200">

  <h4 id="a1.7.3">Part A.1.7.3: Text-Conditional Image-to-image Translation</h4>
  <p>Next, I tried guiding the denoising process by providing a more meaningful prompt. The following is the result of denoising the Campanile at noise levels 1, 3, 5, 7, 10, and 20 (left to right), followed by the original image of the Campanile (rightmost), with the prompt "a wooden pencil". While the images increasingly look like the original Campanile like before, they also have a style of a pencil from the prompt:</p>
  <img src="images/campanile_pencil_1.png" height="200">
  <img src="images/campanile_pencil_3.png" height="200">
  <img src="images/campanile_pencil_5.png" height="200">
  <img src="images/campanile_pencil_7.png" height="200">
  <img src="images/campanile_pencil_10.png" height="200">
  <img src="images/campanile_pencil_20.png" height="200">
  <img src="images/campanile_resized.png" height="200">
  <p>Denoising the image of the coffee cup with the prompt "a ball of yarn":</p>
  <img src="images/coffee_yarn_1.png" height="200">
  <img src="images/coffee_yarn_3.png" height="200">
  <img src="images/coffee_yarn_5.png" height="200">
  <img src="images/coffee_yarn_7.png" height="200">
  <img src="images/coffee_yarn_10.png" height="200">
  <img src="images/coffee_yarn_20.png" height="200">
  <img src="images/im2im_coffee_og.png" height="200">
  <p>Denoising the image of the hot air balloon with the prompt "a pear":</p>
  <img src="images/balloon_pear_1.png" height="200">
  <img src="images/balloon_pear_3.png" height="200">
  <img src="images/balloon_pear_5.png" height="200">
  <img src="images/balloon_pear_7.png" height="200">
  <img src="images/balloon_pear_10.png" height="200">
  <img src="images/balloon_pear_20.png" height="200">
  <img src="images/im2im_balloon_og.png" height="200">

  <h4 id="a1.8">Part A.1.8: Visual Anagrams</h4>
  <p>The following is the resulting anagram from the prompts: "an oil painting of an old man" (rightside up) and "an oil painting of people around a campfire" (upside down):</p>
  <img src="images/man_campfire_anagram_up.png" width="200">
  <img src="images/man_campfire_anagram_down.png" width="200">
  <p>The following is the resulting anagram from the prompts: "a sketch of a frog sitting on a log" (rightside up) and "a sketch of a shoe" (upside down):</p>
  <img src="images/frog_shoe_anagram_up.png" width="200">
  <img src="images/frog_shoe_anagram_down.png" width="200">
  <p>The following is the resulting anagram from the prompts: "a drawing of science lab" (rightside up) and "a drawing of a meadow" (upside down):</p>
  <img src="images/lab_meadow_anagram_up.png" width="200">
  <img src="images/lab_meadow_anagram_down.png" width="200">

  <h4 id="a1.9">Part A.1.9: Hybrid Images</h4>
  <p>The following is a hybrid image between "a skull" (low frequencies) and "a waterfull" (high frequencies):</p>
  <img src="images/skull_waterfall.png" width="200">
  <p>The following is a hybrid image between "a dog" (low frequencies) and "a cat" (high frequencies):</p>
  <img src="images/dog_cat.png" width="200">
  <p>The following is a hybrid image between "a flower" (low frequencies) and "a woman" (high frequencies):</p>
  <img src="images/flower_woman.png" width="200">
  

  <br>
  <br>
  <a href="../index.html">Back to home</a>
</body>
</html>
